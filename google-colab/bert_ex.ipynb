{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#dataset link: https://www.kaggle.com/datasets/dineshpiyasamara/sentiment-analysis-dataset?resource=download\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-1i5k9YFZkY",
        "outputId": "b971b860-bcbc-4fdb-fa67-c0d9db45709e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD1tAAgadBeV"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "5quC9lXxI1UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2id = defaultdict(lambda: len(word2id))\n",
        "PAD = word2id['<pad>']\n",
        "UNK = word2id['<unk>']"
      ],
      "metadata": {
        "id": "R3lF7sVvE8ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_unk():\n",
        "    return UNK"
      ],
      "metadata": {
        "id": "6ZrQpUapEsPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pickle(obj, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "ucuytdP9FyK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MOSEI:\n",
        "    def __init__(self, x, y):\n",
        "\n",
        "        # place holders for the final train/dev/test dataset\n",
        "        self.train = train = []\n",
        "\n",
        "        self.word2id = word2id\n",
        "\n",
        "        num_drop = 0 # a counter to count how many data points went into some processing issues\n",
        "        _words=x\n",
        "\n",
        "        actual_words = []\n",
        "\n",
        "\n",
        "        for i in range(len(_words)):\n",
        "          words = []\n",
        "          for word in _words[i].split(' '):\n",
        "              actual_words.append(word)\n",
        "              words.append(word2id[word])\n",
        "\n",
        "          words = np.asarray(words)\n",
        "          label=y[i]\n",
        "          segment='zuabcedi[8]'\n",
        "          train.append(((words, actual_words), label, segment))\n",
        "\n",
        "\n",
        "        word2id.default_factory = return_unk\n",
        "\n",
        "\n",
        "        # Save pickles\n",
        "        to_pickle(train, '/content/drive/MyDrive/dataset' + '/train.pkl')\n",
        "\n",
        "\n",
        "    def get_data(self):\n",
        "\n",
        "        return self.train, self.word2id\n",
        ""
      ],
      "metadata": {
        "id": "qnpy2BLRoWzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MSADataset(Dataset):\n",
        "    def __init__(self):\n",
        "      data_path='/content/drive/MyDrive/dataset/sentiment_analysis.csv'\n",
        "      x=[]\n",
        "      y=[]\n",
        "      for line in open(data_path):\n",
        "        line=line.strip()\n",
        "        splits = line.split(',')\n",
        "        id=splits[0]\n",
        "        label=splits[1]\n",
        "        sentence=splits[2]\n",
        "        x.append(sentence)\n",
        "        y.append(label)\n",
        "        dataset = MOSEI(x,y)\n",
        "\n",
        "        self.data, self.word2id  = dataset.get_data()\n",
        "        self.len = len(self.data)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "VtC4nYhTnv3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(shuffle=True):\n",
        "    \"\"\"Load DataLoader of given DialogDataset\"\"\"\n",
        "\n",
        "    dataset = MSADataset()\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        Collate functions assume batch = [Dataset[i] for i in index_set]\n",
        "        '''\n",
        "        # for later use we sort the batch in descending order of length\n",
        "        batch = sorted(batch, key=lambda x: x[0][0].shape[0], reverse=True)\n",
        "\n",
        "        # get the data out of the batch - use pad sequence util functions from PyTorch to pad things\n",
        "\n",
        "        labels = torch.cat([torch.from_numpy(sample[1]) for sample in batch], dim=0)\n",
        "        sentences = pad_sequence([torch.LongTensor(sample[0][0]) for sample in batch], padding_value=PAD)\n",
        "\n",
        "        ## BERT-based features input prep\n",
        "\n",
        "        SENT_LEN = sentences.size(0)\n",
        "        # Create bert indices using tokenizer\n",
        "\n",
        "        bert_details = []\n",
        "        for sample in batch:\n",
        "            text = \" \".join(sample[0][1])\n",
        "            encoded_bert_sent = bert_tokenizer.encode_plus(\n",
        "                text, max_length=SENT_LEN+2, add_special_tokens=True, pad_to_max_length=True)\n",
        "            bert_details.append(encoded_bert_sent)\n",
        "\n",
        "\n",
        "        # Bert things are batch_first\n",
        "        bert_sentences = torch.LongTensor([sample[\"input_ids\"] for sample in bert_details])\n",
        "        bert_sentence_types = torch.LongTensor([sample[\"token_type_ids\"] for sample in bert_details])\n",
        "        bert_sentence_att_mask = torch.LongTensor([sample[\"attention_mask\"] for sample in bert_details])\n",
        "\n",
        "\n",
        "        # lengths are useful later in using RNNs\n",
        "        lengths = torch.LongTensor([sample[0][0].shape[0] for sample in batch])\n",
        "\n",
        "        return sentences, labels, lengths, bert_sentences, bert_sentence_types, bert_sentence_att_mask\n",
        "\n",
        "\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=1,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn)\n",
        "\n",
        "    return data_loader"
      ],
      "metadata": {
        "id": "IXKVuegcnazS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_loader = get_loader(shuffle=True)"
      ],
      "metadata": {
        "id": "s4AGgt4KncfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertconfig = BertConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)"
      ],
      "metadata": {
        "id": "4ify9SYMdNWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel = BertModel.from_pretrained('bert-base-uncased', config=bertconfig)"
      ],
      "metadata": {
        "id": "du91vMpUdSGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent, label, lengths, bert_sent, bert_sent_type, bert_sent_mask in train_data_loader:\n",
        "\n",
        "  bert_output = bertmodel(input_ids=bert_sent,\n",
        "                                         attention_mask=bert_sent_mask,\n",
        "                                         token_type_ids=bert_sent_type)\n",
        "  bert_output = bert_output[0]\n",
        "\n",
        "  # masked mean\n",
        "  masked_output = torch.mul(bert_sent_mask.unsqueeze(2), bert_output)\n",
        "  mask_len = torch.sum(bert_sent_mask, dim=1, keepdim=True)\n",
        "  bert_output = torch.sum(masked_output, dim=1, keepdim=False) / mask_len\n",
        "\n",
        "  utterance_text = bert_output\n",
        "\n",
        "  print(utterance_text.shape )"
      ],
      "metadata": {
        "id": "o3_L4PUgdVwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5-lK9FQdcTq"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KS8OkPpZddm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
